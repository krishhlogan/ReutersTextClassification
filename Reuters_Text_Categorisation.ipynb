{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersParser(HTMLParser):\n",
    "    \"\"\"\n",
    "    ReutersParser subclasses HTMLParser and is used to open the SGML\n",
    "    files associated with the Reuters-21578 categorised test collection.\n",
    "\n",
    "    The parser is a generator and will yield a single document at a time.\n",
    "    Since the data will be chunked on parsing, it is necessary to keep \n",
    "    some internal state of when tags have been \"entered\" and \"exited\".\n",
    "    Hence the in_body, in_topics and in_topic_d boolean members.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        \"\"\"\n",
    "        Initialise the superclass (HTMLParser) and reset the parser.\n",
    "        Sets the encoding of the SGML files by default to latin-1.\n",
    "        \"\"\"\n",
    "        html.parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        This is called only on initialisation of the parser class\n",
    "        and when a new topic-body tuple has been generated. It\n",
    "        resets all off the state so that a new tuple can be subsequently\n",
    "        generated.\n",
    "        \"\"\"\n",
    "        self.in_body = False\n",
    "        self.in_topics = False\n",
    "        self.in_topic_d = False\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        \"\"\"\n",
    "        parse accepts a file descriptor and loads the data in chunks\n",
    "        in order to minimise memory usage. It then yields new documents\n",
    "        as they are parsed.\n",
    "        \"\"\"\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        \"\"\"\n",
    "        This method is used to determine what to do when the parser\n",
    "        comes across a particular tag of type \"tag\". In this instance\n",
    "        we simply set the internal state booleans to True if that particular\n",
    "        tag has been found.\n",
    "        \"\"\"\n",
    "        if tag == \"reuters\":\n",
    "            pass\n",
    "        elif tag == \"body\" or tag==\"TITLE\":\n",
    "            self.in_body = True\n",
    "        elif tag == \"topics\":\n",
    "            self.in_topics = True\n",
    "        elif tag == \"d\":\n",
    "            self.in_topic_d = True \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        \"\"\"\n",
    "        This method is used to determine what to do when the parser\n",
    "        finishes with a particular tag of type \"tag\". \n",
    "\n",
    "        If the tag is a  tag, then we remove all \n",
    "        white-space with a regular expression and then append the \n",
    "        topic-body tuple.\n",
    "\n",
    "        If the tag is a  or  tag then we simply set\n",
    "        the internal state to False for these booleans, respectively.\n",
    "\n",
    "        If the tag is a  tag (found within a  tag), then we\n",
    "        append the particular topic to the \"topics\" list and \n",
    "        finally reset it.\n",
    "        \"\"\"\n",
    "        if tag == \"reuters\":\n",
    "            self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "            self.docs.append( (self.topics, self.body) )\n",
    "            self._reset()\n",
    "        elif tag == \"body\" or tag==\"TITLE\":\n",
    "            self.in_body = False\n",
    "        elif tag == \"topics\":\n",
    "            self.in_topics = False\n",
    "        elif tag == \"d\":\n",
    "            self.in_topic_d = False\n",
    "            self.topics.append(self.topic_d)\n",
    "            self.topic_d = \"\"  \n",
    "\n",
    "    def handle_data(self, data):\n",
    "        \"\"\"\n",
    "        The data is simply appended to the appropriate member state\n",
    "        for that particular tag, up until the end closing tag appears.\n",
    "        \"\"\"\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_topic_tags():\n",
    "    \"\"\"\n",
    "    Open the topic list file and import all of the topic names\n",
    "    taking care to strip the trailing \"\\n\" from each word.\n",
    "    \"\"\"\n",
    "    topics = open(\n",
    "        \"reuters/all-topics-strings.lc.txt\", \"r\"\n",
    "    ).readlines()\n",
    "    topics = [t.strip() for t in topics]\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_doc_list_through_topics(topics, docs):\n",
    "    \"\"\"\n",
    "    Reads all of the documents and creates a new list of two-tuples\n",
    "    that contain a single feature entry and the body text, instead of\n",
    "    a list of topics. It removes all geographic features and only \n",
    "    retains those documents which have at least one non-geographic\n",
    "    topic.\n",
    "    \"\"\"\n",
    "    ref_docs = []\n",
    "    for d in docs:\n",
    "        if d[0] == [] or d[0] == \"\":\n",
    "            continue\n",
    "        for t in d[0]:\n",
    "            if t in topics:\n",
    "                d_tup = (t, d[1])\n",
    "                ref_docs.append(d_tup)\n",
    "                break\n",
    "    return ref_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_training_data(docs):\n",
    "    \"\"\"\n",
    "    Creates a document corpus list (by stripping out the\n",
    "    class labels), then applies the TF-IDF transform to this\n",
    "    list. \n",
    "\n",
    "    The function returns both the class label vector (y) and \n",
    "    the corpus token/feature matrix (X).\n",
    "    \"\"\"\n",
    "    # Create the training data class labels\n",
    "    y = [d[0] for d in docs]\n",
    "    \n",
    "    # Create the document corpus list\n",
    "    corpus = [d[1] for d in docs]\n",
    "\n",
    "    # Create the TF-IDF vectoriser and transform the corpus\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    joblib.dump(vectorizer, 'tfidf.pkl') \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_raw_data():\n",
    "    \"\"\"\n",
    "    This function is used to parse through the entire .sgm files and obtain the dataset as a list\n",
    "    \"\"\"\n",
    "    files = [\"reuters/reut2-%03d.sgm\" % r for r in range(0, 22)]\n",
    "    parser = ReutersParser()\n",
    "\n",
    "    # Parse the document and force all generated docs into\n",
    "    # a list so that it can be printed out to the console\n",
    "    docs = []\n",
    "    for fn in files:\n",
    "        for d in parser.parse(open(fn, 'rb')):\n",
    "            docs.append(d)\n",
    "\n",
    "    # Obtain the topic tags and filter docs through it \n",
    "    topics = obtain_topic_tags()\n",
    "    ref_docs = filter_doc_list_through_topics(topics, docs)\n",
    "    return ref_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(raw_data):\n",
    "    \"\"\"\n",
    "    This function removes all the stopwords and applies stemming on the sentences in the dataset and returns the vectorised form of\n",
    "    data after applying tfidfvectoriser\n",
    "    \"\"\"\n",
    "    temp=[]\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for i in range(len(raw_data)):\n",
    "        tokens=word_tokenize(raw_data[i][1])\n",
    "        ps = PorterStemmer() \n",
    "        filtered_sentence = tuple(ps.stem(w) for w in tokens if not w in stop_words)\n",
    "        #joblib.\n",
    "        temp.append((raw_data[i][0],\" \".join(filtered_sentence)))\n",
    "    return create_tfidf_training_data(temp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_text(data):\n",
    "    \"\"\"\n",
    "    This function is used to preprocess the given input text document and returns vectorised form of the given text data .\n",
    "    \"\"\"\n",
    "    from nltk import  sent_tokenize\n",
    "    data = sent_tokenize(data)\n",
    "    vect = joblib.load('tfidf.pkl')\n",
    "    t = vect.transform(data)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(data,model):\n",
    "    \"\"\"\n",
    "    This function predicts the class of input text document (in the form of string). This takens in the vectorised form of input \n",
    "    data obtained as output of preprocess_test_text function and returns the predicted class.\n",
    "    \"\"\"\n",
    "    return model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data():\n",
    "    \"\"\"\n",
    "    This function returns the train and test split of dataset used for training the model\n",
    "    \"\"\"\n",
    "    raw_data=get_raw_data()\n",
    "    X,y=preprocess_data(raw_data)\n",
    "    return train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the training-test split of the data\n",
    "X_train, X_test, y_train, y_test =prepare_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    This function trains the Stochaistic Gradient Descent Classifier with the X_train and y_train data( training data set) and returns the model\n",
    "    \"\"\"\n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.SGDClassifier(max_iter=10000,tol=1e-3)\n",
    "    model=clf.fit(X_train, y_train)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_score():\n",
    "    \"\"\"\n",
    "    This function gets the accuracy score of the model by validating the model with test data set\n",
    "    \"\"\"\n",
    "    pred=predict_data(X_test,model)\n",
    "    return sklearn.metrics.accuracy_score(y_test,pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cocoa',\n",
       " 'Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review. The dry period means the temporao will be late this year. Arrivals for the week ended February 22 were 155,221 bags of 60 kilos making a cumulative total for the season of 5.93 mln against 5.81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures. Comissaria Smith said there is still some doubt as to how much old crop cocoa is still available as harvesting has practically come to an end. With total Bahia crop estimates around 6.4 mln bags and sales standing at almost 6.2 mln there are a few hundred thousand bags still in the hands of farmers, middlemen, exporters and processors. There are doubts as to how much of this cocoa would be fit for export as shippers are now experiencing dificulties in obtaining +Bahia superior+ certificates. In view of the lower quality over recent weeks farmers have sold a good part of their cocoa held on consignment. Comissaria Smith said spot bean prices rose to 340 to 350 cruzados per arroba of 15 kilos. Bean shippers were reluctant to offer nearby shipment and only limited sales were booked for March shipment at 1,750 to 1,780 dlrs per tonne to ports to be named. New crop sales were also light and all to open ports with June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs per tonne FOB. Routine sales of butter were made. March/April sold at 4,340, 4,345 and 4,350 dlrs. April/May butter went at 2.27 times New York May, June/July at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at 2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and 2.27 times New York Dec, Comissaria Smith said. Destinations were the U.S., Covertible currency areas, Uruguay and open ports. Cake sales were registered at 785 to 995 dlrs for March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times New York Dec for Oct/Dec. Buyers were the U.S., Argentina, Uruguay and convertible currency areas. Liquor sales were limited with March/April selling at 2,325 and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith said. Total Bahia sales are currently estimated at 6.13 mln bags against the 1986/87 crop and 1.06 mln bags against the 1987/88 crop. Final figures for the period to February 28 are expected to be published by the Brazilian Cocoa Trade Commission after carnival which ends midday on February 27. Reuter ')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=raw_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cocoa', 'cocoa', 'grain', 'cocoa', 'cocoa', 'grain', 'cocoa',\n",
       "       'cocoa', 'trade', 'grain', 'grain', 'grain', 'crude', 'earn',\n",
       "       'trade', 'earn', 'grain', 'earn', 'grain', 'cocoa', 'acq'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data(preprocess_test_text(text),model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 85.26824978012313 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score :\",get_accuracy_score()*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
